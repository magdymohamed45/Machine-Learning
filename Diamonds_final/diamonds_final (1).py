# -*- coding: utf-8 -*-
"""Diamonds_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q8zqG9uE8bVDjyOvGxEuwnQ4oFR89cc-

## Importing the libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='whitegrid')

import warnings
warnings.filterwarnings('ignore')

"""## Import the dataset"""

data=pd.read_csv('/content/Untitled Folder/diamonds.csv')
data.head()

"""## Overview of data"""

len(data)

data.shape[0]

data.info()

data.describe()

"""## Data Cleaning and Preprocessing"""

data = data.drop(["Unnamed: 0"],axis=1)
data.head()

data.isnull().sum()

data['volume'] = data['x']*data['y']*data['z']
data.drop(['x', 'y', 'z'], axis=1, inplace=True)
data.head()

"""## Correlation Matrix"""

import seaborn as sns
plt.figure(figsize=(12, 8))
sns.heatmap(data.drop(columns=['cut', 'color', 'clarity']).corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Feature Correlation Matrix')
plt.show()

"""Outlier Analysis"""

data.plot(kind='box',figsize=(15,10),subplots=True,layout=(3,3))
plt.show()

def remove_outliers(data, col):
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
    print("Outliers:", outliers)

    cleaned_data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]
    return cleaned_data


numerical_cols = data.select_dtypes(include=np.number).columns
for col in numerical_cols:
    data = remove_outliers(data, col)

data.shape

"""Analysing Numerical and Categorical data"""

numerical_cols = data.select_dtypes(include=np.number).columns.to_list()
categorical_cols = data.select_dtypes(exclude=np.number).columns.to_list()

numerical_cols

categorical_cols

data['cut'].value_counts()

data['color'].value_counts()

data['clarity'].value_counts()

sns.catplot(x='clarity', data=data, kind='count',aspect=2.5)

sns.catplot(x='color', data=data, kind='count',aspect=2.5)

sns.catplot(x='cut', data=data, kind='count',aspect=2.5)

"""Data Visualization"""

sns.scatterplot(data=data, x=data['volume'], y=data['carat'])

sns.scatterplot(data=data, x=data['volume'], y=data['price'])

sns.scatterplot(data=data, x=data['volume'], y=data['table'])

sns.pairplot(data)

"""## Split data  X and y"""

X = data.drop(columns=['price'])
y = data['price']

"""## Encoding the Independent Variable"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
categorical_cols = ['cut', 'color', 'clarity']
ct = ColumnTransformer(
    transformers=[
        ('encoder', OneHotEncoder(drop='first'), categorical_cols)
    ],
    remainder='passthrough'
)

X = ct.fit_transform(X)

"""## Spliting Train-Test"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

"""## Training the Random Forest Regressor on the whole dataset"""

from sklearn.ensemble import RandomForestRegressor
model= RandomForestRegressor(n_estimators = 73, random_state = 44)
model.fit(X_train,y_train)

"""## Evaluating Model"""

from sklearn.metrics import r2_score, mean_squared_error
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"MSE: {mse}")
print(f"R2: {r2}")

"""## Visualising the Random Forest Regression results (higher resolution)"""

plt.scatter(y_test, y_pred, alpha=0.5 ,color = 'red')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color = 'blue')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Truth Random Forest Regression')
plt.grid(True)

